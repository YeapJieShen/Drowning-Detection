{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97d7eab",
   "metadata": {},
   "source": [
    "# Detection with CNNClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f78dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Last Updated**: 2025-04-19 13:28:00\n",
       "\n",
       "**Python Version**: 3.11.11  \n",
       "**OS**: Windows 10.0.26100  \n",
       "**Architecture**: 64bit  \n",
       "**Hostname**: ShenLaptop  \n",
       "**Processor**: Intel64 Family 6 Model 186 Stepping 3, GenuineIntel  \n",
       "**RAM Size**: 15.65 GB  \n",
       "  \n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "\n",
    "from utils.system import display_system_info\n",
    "\n",
    "display_system_info(markdown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54e8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "IDX_TO_CLASS = {\n",
    "    0: 'swimming',\n",
    "    1: 'treadwater',\n",
    "    2: 'drowning'\n",
    "}\n",
    "\n",
    "def xywhn_to_xyxy(image, x_n, y_n, w_n, h_n):\n",
    "    # cv2 images in HWC\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    x1 = int((x_n - w_n / 2) * width)\n",
    "    y1 = int((y_n - h_n / 2) * height)\n",
    "    x2 = int((x_n + w_n / 2) * width)\n",
    "    y2 = int((y_n + h_n / 2) * height)\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def cop_save_roi(images_folder, labels_folder, output_folder, idx_to_class):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for class_id, class_name in idx_to_class.items():\n",
    "        os.makedirs(os.path.join(output_folder, class_name), exist_ok=True)\n",
    "        \n",
    "    class_counters = {class_id: 0 for class_id in idx_to_class.keys()}\n",
    "\n",
    "    for image_file in os.listdir(images_folder):\n",
    "        if image_file.endswith('.jpg'):\n",
    "            base_name = os.path.splitext(image_file)[0]\n",
    "            \n",
    "            label_file = os.path.join(labels_folder, f\"{base_name}.txt\")\n",
    "            image_path = os.path.join(images_folder, image_file)\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            # Read the label file\n",
    "            with open(label_file, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            # Loop through each line in the label file\n",
    "            for line in lines:\n",
    "                values = line.strip().split()\n",
    "                class_id = int(values[0])\n",
    "                x_n, y_n, w_n, h_n = map(float, values[1:])\n",
    "\n",
    "                x1, y1, x2, y2 = xywhn_to_xyxy(image, x_n, y_n, w_n, h_n)\n",
    "\n",
    "                roi = image[y1:y2, x1:x2]\n",
    "\n",
    "                cv2.imwrite(os.path.join(output_folder, idx_to_class[class_id], f\"{str(class_counters[class_id]).zfill(6)}.jpg\"), roi)\n",
    "\n",
    "                class_counters[class_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8135f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'val']:\n",
    "    images_folder = os.path.join(os.getenv('RAW_DATA_DIR'), 'images', split)\n",
    "    labels_folder = os.path.join(os.getenv('RAW_DATA_DIR'), 'labels', split)\n",
    "    output_folder = os.path.join(os.getenv('ROI_DATA_DIR'), split)\n",
    "\n",
    "    cop_save_roi(images_folder, labels_folder, output_folder, IDX_TO_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733b435",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclassify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchClassifier\n\u001b[32m      3\u001b[39m model_config = {\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m3\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnum_blocks\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m4\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minput_shape\u001b[39m\u001b[33m'\u001b[39m: (\u001b[32m3\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m128\u001b[39m)\n\u001b[32m     12\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model = \u001b[43mTorchClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCNNClassifier\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\Drowning-Detection\\src\\classify\\classifier.py:31\u001b[39m, in \u001b[36mTorchClassifier.__init__\u001b[39m\u001b[34m(self, model, model_path, config, device, verbose)\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m.load(model_path)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m config:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\Drowning-Detection\\src\\classify\\classifier.py:40\u001b[39m, in \u001b[36mTorchClassifier._new\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_new\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mConfiguration must be a dictionary!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\Drowning-Detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\Drowning-Detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\Drowning-Detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 903 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\Drowning-Detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\Drowning-Detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\Drowning-Detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\Drowning-Detection\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:319\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    318\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    323\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "from classify import TorchClassifier\n",
    "\n",
    "model_config = {\n",
    "    'num_classes': 3,\n",
    "    'num_blocks': 4,\n",
    "    'first_out_channel': 32,\n",
    "    'out_channel_multiplier': 2,\n",
    "    'kernel_size': 3,\n",
    "    'stride': 1,\n",
    "    'padding': 1,\n",
    "    'input_shape': (3, 128, 128)\n",
    "}\n",
    "\n",
    "model = TorchClassifier(model='CNNClassifier', config=model_config, device='auto', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60eb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch       Loss   Accuracy    Macro(P          R        F1) Weighted(P          R        F1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      0.677       69.9     0.7012      0.703     0.7016     0.7012      0.703     0.7016: 100%|██████████| 56/56 [01:13<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                0.4093: 100%|██████████| 37/37 [00:21<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    drowning     0.8344    0.6923    0.7568       182\n",
      "    swimming     0.8635    0.9623    0.9102       690\n",
      "  treadwater     0.8903    0.7404    0.8084       285\n",
      "\n",
      "    accuracy                         0.8652      1157\n",
      "   macro avg     0.8627    0.7983    0.8251      1157\n",
      "weighted avg     0.8655    0.8652    0.8610      1157\n",
      "\n",
      "      Epoch       Loss   Accuracy    Macro(P          R        F1) Weighted(P          R        F1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5     0.3847       84.3     0.8469     0.8478     0.8471     0.8469     0.8478     0.8471: 100%|██████████| 56/56 [01:42<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                0.2797: 100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    drowning     0.9412    0.6154    0.7442       182\n",
      "    swimming     0.9307    0.9536    0.9420       690\n",
      "  treadwater     0.7946    0.9228    0.8539       285\n",
      "\n",
      "    accuracy                         0.8928      1157\n",
      "   macro avg     0.8888    0.8306    0.8467      1157\n",
      "weighted avg     0.8988    0.8928    0.8892      1157\n",
      "\n",
      "      Epoch       Loss   Accuracy    Macro(P          R        F1) Weighted(P          R        F1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5     0.2971      87.13     0.8759     0.8762      0.876     0.8759     0.8762      0.876: 100%|██████████| 56/56 [01:11<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                0.3111: 100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    drowning     0.8671    0.8242    0.8451       182\n",
      "    swimming     0.9774    0.8768    0.9244       690\n",
      "  treadwater     0.7452    0.9544    0.8369       285\n",
      "\n",
      "    accuracy                         0.8876      1157\n",
      "   macro avg     0.8632    0.8851    0.8688      1157\n",
      "weighted avg     0.9028    0.8876    0.8904      1157\n",
      "\n",
      "      Epoch       Loss   Accuracy    Macro(P          R        F1) Weighted(P          R        F1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5       0.29      88.42      0.889     0.8893     0.8891      0.889     0.8893     0.8891: 100%|██████████| 56/56 [01:12<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                0.1763: 100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    drowning     0.9222    0.8462    0.8825       182\n",
      "    swimming     0.9667    0.9667    0.9667       690\n",
      "  treadwater     0.8933    0.9404    0.9162       285\n",
      "\n",
      "    accuracy                         0.9412      1157\n",
      "   macro avg     0.9274    0.9177    0.9218      1157\n",
      "weighted avg     0.9416    0.9412    0.9410      1157\n",
      "\n",
      "      Epoch       Loss   Accuracy    Macro(P          R        F1) Weighted(P          R        F1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5     0.2905      88.65     0.8913     0.8915     0.8914     0.8913     0.8915     0.8914: 100%|██████████| 56/56 [01:11<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                0.2219: 100%|██████████| 37/37 [00:20<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    drowning     0.9341    0.8571    0.8940       182\n",
      "    swimming     0.9162    0.9826    0.9483       690\n",
      "  treadwater     0.9320    0.8175    0.8710       285\n",
      "\n",
      "    accuracy                         0.9222      1157\n",
      "   macro avg     0.9274    0.8858    0.9044      1157\n",
      "weighted avg     0.9229    0.9222    0.9207      1157\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class RandomAffineWithInpainting:\n",
    "    def __init__(self, degrees=10, translate=(0.1, 0.1), radius=3):\n",
    "        self.affine = transforms.RandomAffine(degrees=degrees, translate=translate)\n",
    "        self.inpaint_radius = radius\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Step 1: Apply affine transform (returns PIL image)\n",
    "        img = self.affine(img)\n",
    "\n",
    "        # Step 2: Convert to NumPy array\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        return Image.fromarray(img_np)\n",
    "\n",
    "        # Step 3: Create mask where pixels are black\n",
    "        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "        mask = cv2.inRange(gray, 0, 1)\n",
    "\n",
    "        # Step 4: Inpaint using OpenCV\n",
    "        inpainted_np = cv2.inpaint(img_np, mask, self.inpaint_radius, cv2.INPAINT_TELEA)\n",
    "\n",
    "        # Step 5: Convert back to PIL\n",
    "        img_inpainted = Image.fromarray(inpainted_np)\n",
    "\n",
    "        return img_inpainted\n",
    "    \n",
    "class RGBToHSV(object):\n",
    "    def __call__(self, img):\n",
    "        # Ensure the image is in PIL format before converting\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = transforms.ToPILImage()(img)  # Convert tensor to PIL image\n",
    "        \n",
    "        # Convert the image to HSV using PIL\n",
    "        img_hsv = img.convert(\"HSV\")\n",
    "        \n",
    "        return img_hsv\n",
    "    \n",
    "class VBlurring(object):\n",
    "    def __call__(self, img):\n",
    "        # Ensure the image is in PIL format before converting\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = transforms.ToPILImage()(img)\n",
    "\n",
    "        # apply blurring to the V channel of the HSV image\n",
    "        h, s, v = cv2.split(np.array(img))\n",
    "\n",
    "        v_blurred = cv2.blur(v, (7, 7))  # Apply 3x3 average blur to the V channel\n",
    "\n",
    "        img_hsv_blurred = cv2.merge([h, s, v_blurred])\n",
    "\n",
    "        return Image.fromarray(img_hsv_blurred)\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    RandomAffineWithInpainting(degrees=0, translate=(0.2, 0.2))\n",
    "])\n",
    "\n",
    "enhance_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    RGBToHSV(),\n",
    "    VBlurring(),\n",
    "    # May addd certain non-deterministic augmentations here to improve generalisability\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    RGBToHSV(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "results = model.train(\n",
    "    data_path=os.getenv('ROI_DATA_DIR'),\n",
    "    imbalance=True,\n",
    "    fraction=1,\n",
    "    val_test_ratio=0.5,\n",
    "    input_size=128,\n",
    "    optimizer='Adam',\n",
    "    lr=1e-4,\n",
    "    aug_transform=aug_transform,\n",
    "    enhance_transform=enhance_transform,\n",
    "    val_transform=val_transform,\n",
    "    batch_size=32,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f47d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'loss': 0.677042370329478,\n",
       "   'accuracy': 69.90400903444382,\n",
       "   'macro_f1': 0.7016033907871405,\n",
       "   'weighted_f1': 0.7016033907871405,\n",
       "   'macro_precision': 0.7012251214055705,\n",
       "   'weighted_precision': 0.7012251214055704,\n",
       "   'macro_recall': 0.7030096536059057,\n",
       "   'weighted_recall': 0.7030096536059057},\n",
       "  {'loss': 0.38468805767063585,\n",
       "   'accuracy': 84.30265386787126,\n",
       "   'macro_f1': 0.8470559795932763,\n",
       "   'weighted_f1': 0.8470559795932763,\n",
       "   'macro_precision': 0.8468855401125621,\n",
       "   'weighted_precision': 0.8468855401125621,\n",
       "   'macro_recall': 0.8478137421919364,\n",
       "   'weighted_recall': 0.8478137421919364},\n",
       "  {'loss': 0.29706055098878487,\n",
       "   'accuracy': 87.12591756070017,\n",
       "   'macro_f1': 0.8760057325695411,\n",
       "   'weighted_f1': 0.8760057325695411,\n",
       "   'macro_precision': 0.8758951375618041,\n",
       "   'weighted_precision': 0.8758951375618043,\n",
       "   'macro_recall': 0.8762067007382169,\n",
       "   'weighted_recall': 0.8762067007382169},\n",
       "  {'loss': 0.29004918318241835,\n",
       "   'accuracy': 88.42461885940146,\n",
       "   'macro_f1': 0.8891429671542238,\n",
       "   'weighted_f1': 0.8891429671542237,\n",
       "   'macro_precision': 0.8890423854912042,\n",
       "   'weighted_precision': 0.8890423854912041,\n",
       "   'macro_recall': 0.8892674616695059,\n",
       "   'weighted_recall': 0.889267461669506},\n",
       "  {'loss': 0.2904532960216914,\n",
       "   'accuracy': 88.65047995482779,\n",
       "   'macro_f1': 0.8913704701935067,\n",
       "   'weighted_f1': 0.8913704701935067,\n",
       "   'macro_precision': 0.8912601378160856,\n",
       "   'weighted_precision': 0.8912601378160856,\n",
       "   'macro_recall': 0.8915388983532084,\n",
       "   'weighted_recall': 0.8915388983532084}],\n",
       " [{'loss': 0.4092931227909552,\n",
       "   'drowning': {'precision': 0.8344370860927153,\n",
       "    'recall': 0.6923076923076923,\n",
       "    'f1-score': 0.7567567567567568,\n",
       "    'support': 182.0},\n",
       "   'swimming': {'precision': 0.8634590377113134,\n",
       "    'recall': 0.9623188405797102,\n",
       "    'f1-score': 0.910212474297464,\n",
       "    'support': 690.0},\n",
       "   'treadwater': {'precision': 0.890295358649789,\n",
       "    'recall': 0.7403508771929824,\n",
       "    'f1-score': 0.8084291187739464,\n",
       "    'support': 285.0},\n",
       "   'accuracy': 0.8651685393258427,\n",
       "   'macro avg': {'precision': 0.8627304941512725,\n",
       "    'recall': 0.7983258033601284,\n",
       "    'f1-score': 0.8251327832760557,\n",
       "    'support': 1157.0},\n",
       "   'weighted avg': {'precision': 0.8655042894596977,\n",
       "    'recall': 0.8651685393258427,\n",
       "    'f1-score': 0.8610014138682408,\n",
       "    'support': 1157.0}},\n",
       "  {'loss': 0.2797331000502045,\n",
       "   'drowning': {'precision': 0.9411764705882353,\n",
       "    'recall': 0.6153846153846154,\n",
       "    'f1-score': 0.7441860465116279,\n",
       "    'support': 182.0},\n",
       "   'swimming': {'precision': 0.9306930693069307,\n",
       "    'recall': 0.9536231884057971,\n",
       "    'f1-score': 0.9420186113099499,\n",
       "    'support': 690.0},\n",
       "   'treadwater': {'precision': 0.7945619335347432,\n",
       "    'recall': 0.9228070175438596,\n",
       "    'f1-score': 0.8538961038961039,\n",
       "    'support': 285.0},\n",
       "   'accuracy': 0.8928262748487468,\n",
       "   'macro avg': {'precision': 0.8888104911433031,\n",
       "    'recall': 0.8306049404447574,\n",
       "    'f1-score': 0.8467002539058939,\n",
       "    'support': 1157.0},\n",
       "   'weighted avg': {'precision': 0.8988094092707372,\n",
       "    'recall': 0.8928262748487468,\n",
       "    'f1-score': 0.8891919549519198,\n",
       "    'support': 1157.0}},\n",
       "  {'loss': 0.3111348732097729,\n",
       "   'drowning': {'precision': 0.8670520231213873,\n",
       "    'recall': 0.8241758241758241,\n",
       "    'f1-score': 0.8450704225352113,\n",
       "    'support': 182.0},\n",
       "   'swimming': {'precision': 0.9773828756058158,\n",
       "    'recall': 0.8768115942028986,\n",
       "    'f1-score': 0.9243697478991597,\n",
       "    'support': 690.0},\n",
       "   'treadwater': {'precision': 0.7452054794520548,\n",
       "    'recall': 0.9543859649122807,\n",
       "    'f1-score': 0.8369230769230769,\n",
       "    'support': 285.0},\n",
       "   'accuracy': 0.8876404494382022,\n",
       "   'macro avg': {'precision': 0.8632134593930859,\n",
       "    'recall': 0.8851244610970012,\n",
       "    'f1-score': 0.8687877491191492,\n",
       "    'support': 1157.0},\n",
       "   'weighted avg': {'precision': 0.9028359671736741,\n",
       "    'recall': 0.8876404494382022,\n",
       "    'f1-score': 0.8903552462185872,\n",
       "    'support': 1157.0}},\n",
       "  {'loss': 0.17629040868298426,\n",
       "   'drowning': {'precision': 0.9221556886227545,\n",
       "    'recall': 0.8461538461538461,\n",
       "    'f1-score': 0.8825214899713467,\n",
       "    'support': 182.0},\n",
       "   'swimming': {'precision': 0.9666666666666667,\n",
       "    'recall': 0.9666666666666667,\n",
       "    'f1-score': 0.9666666666666667,\n",
       "    'support': 690.0},\n",
       "   'treadwater': {'precision': 0.8933333333333333,\n",
       "    'recall': 0.9403508771929825,\n",
       "    'f1-score': 0.9162393162393162,\n",
       "    'support': 285.0},\n",
       "   'accuracy': 0.9412273120138289,\n",
       "   'macro avg': {'precision': 0.9273852295409183,\n",
       "    'recall': 0.9177237966711651,\n",
       "    'f1-score': 0.9218091576257765,\n",
       "    'support': 1157.0},\n",
       "   'weighted avg': {'precision': 0.9416009812699578,\n",
       "    'recall': 0.9412273120138289,\n",
       "    'f1-score': 0.9410087435635179,\n",
       "    'support': 1157.0}},\n",
       "  {'loss': 0.22185370258080797,\n",
       "   'drowning': {'precision': 0.9341317365269461,\n",
       "    'recall': 0.8571428571428571,\n",
       "    'f1-score': 0.8939828080229226,\n",
       "    'support': 182.0},\n",
       "   'swimming': {'precision': 0.9162162162162162,\n",
       "    'recall': 0.9826086956521739,\n",
       "    'f1-score': 0.9482517482517483,\n",
       "    'support': 690.0},\n",
       "   'treadwater': {'precision': 0.932,\n",
       "    'recall': 0.8175438596491228,\n",
       "    'f1-score': 0.8710280373831776,\n",
       "    'support': 285.0},\n",
       "   'accuracy': 0.9222126188418324,\n",
       "   'macro avg': {'precision': 0.9274493175810541,\n",
       "    'recall': 0.8857651374813846,\n",
       "    'f1-score': 0.9044208645526162,\n",
       "    'support': 1157.0},\n",
       "   'weighted avg': {'precision': 0.9229223554339614,\n",
       "    'recall': 0.9222126188418324,\n",
       "    'f1-score': 0.9206927986241003,\n",
       "    'support': 1157.0}}])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53d840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to C:\\Users\\hp\\Downloads\\Drowning-Detection\\models\\CNN\\test.pt\n"
     ]
    }
   ],
   "source": [
    "model.save(\n",
    "    os.path.join(os.getenv('CNN_MODEL_DIR'), 'test.pt')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef083eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model prediction here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
