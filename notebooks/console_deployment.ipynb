{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9a2255",
   "metadata": {},
   "source": [
    "# Console Deployment\n",
    "\n",
    "This notebook showcases how the **Drowning Detection System** is applied directly within the notebook for:\n",
    "\n",
    "- **Video File Inference**  \n",
    "  We run inference on pre-recorded swimming pool footage to detect and classify human behaviors such as *swimming*, *treading water*, and *drowning*.\n",
    "\n",
    "- **Live Webcam Streaming**  \n",
    "  The system captures frames from a live webcam feed, overlays predictions (with timestamp), and streams the output back in real time. This emulates real-world deployment for poolside surveillance or lifeguard assist systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33dbf99b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSRC_DIR\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msystem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display_system_info\n\u001b[0;32m     12\u001b[0m display_system_info(markdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "\n",
    "from utils.system import display_system_info\n",
    "\n",
    "display_system_info(markdown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43636fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from classify import TorchClassifier\n",
    "import cv2\n",
    "from pygame import mixer\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242da1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ROOT = r\"C:\\Users\\PC\\Downloads\\Jie Shen\\Drowning-Detection\"\n",
    "\n",
    "# YOLO_PATH = os.path.join(PROJECT_ROOT, r\"models\\detection\\YOLO\\yolo11n-finetuned\\human_detection_yolo11n.pt\")\n",
    "YOLO_PATH = os.path.join(PROJECT_ROOT, r\"models\\detection\\YOLO\\jrom_yolo11n.pt\")\n",
    "YOLO_PATH = r\"C:\\Users\\PC\\Downloads\\Jie Shen\\Drowning-Detection\\models\\detection\\YOLO\\best.pt\"\n",
    "YOLO_PATH = r\"C:\\Users\\PC\\Downloads\\Jie Shen\\Drowning-Detection\\models\\detection\\YOLO\\yolo11n-finetuned\\weights\\best.pt\"\n",
    "YOLO_PATH = r\"C:\\Users\\PC\\Downloads\\Jie Shen\\Drowning-Detection\\models\\detection\\YOLO\\yolo11n-finetuned\\weights\\epoch36.pt\"\n",
    "# YOLO_PATH = os.path.join(PROJECT_ROOT, r\"models\\detection\\YOLO\\yolo11m.pt\")\n",
    "# YOLO_PATH = r\"C:\\Users\\PC\\Downloads\\Jie Shen\\Drowning-Detection\\notebooks\\detection\\yolo11n.pt\"\n",
    "# YOLO_PATH = r\"yolo11n.pt\"\n",
    "\n",
    "CNN_PATH = os.path.join(PROJECT_ROOT, r\"models\\classification\\CNN\\test.pt\")\n",
    "SIREN_PATH = os.path.join(PROJECT_ROOT, r\"data\\audio\\siren.wav\")\n",
    "VIDEO_PATH = os.path.join(PROJECT_ROOT, r\"data\\videos\\manyswimmers.mp4\")\n",
    "\n",
    "LOOKBACK = 20\n",
    "SENSITIVITY = 0.5\n",
    "\n",
    "mixer.init()\n",
    "mixer.music.load(SIREN_PATH)\n",
    "\n",
    "detection_model = YOLO(YOLO_PATH)\n",
    "classification_model = TorchClassifier(model=\"CNNClassifier\", model_path=CNN_PATH)\n",
    "siren = mixer.music\n",
    "\n",
    "obj_confs_info = {}\n",
    "class_confs_info = {}\n",
    "predicted_classes_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_activity(roi):\n",
    "    import torch\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "\n",
    "    class RGBToHSV:\n",
    "        def __call__(self, img):\n",
    "            # Ensure the image is in PIL format before converting\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                img = transforms.ToPILImage()(img)  # Convert tensor to PIL image\n",
    "\n",
    "            # Convert the image to HSV using PIL\n",
    "            img_hsv = img.convert(\"HSV\")\n",
    "\n",
    "            return img_hsv\n",
    "\n",
    "    roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        RGBToHSV(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    prediction = classification_model(\n",
    "        img=roi_pil,\n",
    "        transform=transform,\n",
    "        prob=True\n",
    "    )\n",
    "\n",
    "    return prediction.cpu().numpy()\n",
    "\n",
    "\n",
    "def detect_drowning(frame, sensitivity, lookback, activate_siren):\n",
    "    CLASS_IDX_TO_NAME = {\n",
    "        0: 'drowning',\n",
    "        1: 'swimming',\n",
    "        2: 'treadwater'\n",
    "    }\n",
    "\n",
    "    CLASS_NAME_TO_IDX = {\n",
    "        value: key\n",
    "        for key, value in CLASS_IDX_TO_NAME.items()\n",
    "    }\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    result = detection_model.track(\n",
    "        frame, persist=True, tracker=\"botsort.yaml\", verbose=False)[0]\n",
    "\n",
    "    if result.boxes and result.boxes.id is not None:\n",
    "        obj_boxes = result.boxes.xyxy.cpu().numpy()\n",
    "        obj_ids = result.boxes.id.cpu().numpy()\n",
    "        obj_confs = result.boxes.conf.cpu().numpy()\n",
    "\n",
    "        for box, obj_id, obj_conf in zip(obj_boxes, obj_ids, obj_confs):\n",
    "            if obj_id not in obj_confs_info or obj_id not in class_confs_info or obj_id not in predicted_classes_info:\n",
    "                current_obj_conf = None\n",
    "                current_class_vec = None\n",
    "                predicted_class_history = deque(maxlen=lookback)\n",
    "            else:\n",
    "                current_obj_conf = obj_confs_info[obj_id]\n",
    "                current_class_vec = class_confs_info[obj_id]\n",
    "                predicted_class_history = predicted_classes_info[obj_id]\n",
    "\n",
    "            new_obj_conf = obj_confs_info[obj_id] = (\n",
    "                (1 - sensitivity) * current_obj_conf + sensitivity * obj_conf\n",
    "                if current_obj_conf is not None else\n",
    "                obj_conf\n",
    "            )\n",
    "\n",
    "            # if new_obj_conf < 0.5:\n",
    "            #     continue\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "            class_vec = classify_activity(roi)\n",
    "\n",
    "            new_class_vec = class_confs_info[obj_id] = (\n",
    "                (1 - sensitivity) * current_class_vec + sensitivity * class_vec\n",
    "                if current_class_vec is not None else\n",
    "                class_vec\n",
    "            )\n",
    "\n",
    "            predicted_class_idx = int(np.argmax(new_class_vec))\n",
    "            predicted_class_history.append(predicted_class_idx)\n",
    "            predicted_classes_info[obj_id] = predicted_class_history\n",
    "\n",
    "            if len(predicted_class_history) < lookback:\n",
    "                continue\n",
    "\n",
    "            predicted_class_name = CLASS_IDX_TO_NAME[max(\n",
    "                set(predicted_class_history), key=predicted_class_history.count)]\n",
    "            # TODO: For testing only\n",
    "            # predicted_class_name = \"drowning\" if predicted_class_name == \"treadwater\" else predicted_class_name\n",
    "            # Normalize to 0â€“1\n",
    "            drowning_prob = min(max(new_class_vec[CLASS_NAME_TO_IDX[\"drowning\"]], 0), 100)\n",
    "\n",
    "            # Interpolate: red = (255, 0, 0), green = (0, 255, 0)\n",
    "            bgr = 0, int(255 * (1 - drowning_prob)), int(255 * drowning_prob)\n",
    "\n",
    "            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), bgr, 2)\n",
    "            cv2.putText(annotated_frame, f'{new_obj_conf:.2f} {int(obj_id)} {predicted_class_name} {new_class_vec[predicted_class_idx] * 100:.2f}%', (\n",
    "                x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, bgr, 2)\n",
    "\n",
    "            if predicted_class_name == \"drowning\":\n",
    "                activate_siren = True\n",
    "\n",
    "                if activate_siren and not siren.get_busy():\n",
    "                    siren.play()\n",
    "\n",
    "                # log_drowning_info(obj_id, drowning_prob, roi)\n",
    "\n",
    "    if not activate_siren and siren.get_busy():\n",
    "        siren.fadeout(1000)\n",
    "\n",
    "    cv2.putText(annotated_frame, f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "                (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    return annotated_frame, activate_siren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac3d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to read from the camera.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access the camera.\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to read from the camera.\")\n",
    "        break\n",
    "\n",
    "    activate_siren = False\n",
    "\n",
    "    annotated_frame, activate_siren = detect_drowning(frame, SENSITIVITY, LOOKBACK, activate_siren)\n",
    "\n",
    "    cv2.imshow(\"Console Deployment\", annotated_frame)\n",
    "\n",
    "    if not activate_siren and siren.get_busy():\n",
    "        siren.fadeout(1000)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "if not activate_siren and siren.get_busy():\n",
    "    siren.fadeout(1000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d731a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
