{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "About the dataset :\n",
    "1. Dataset diversity - various postures (treading water, breaststroke, backstroke, freestyle, group configurations with 2/3/4 individuals, were included)\n",
    "2. 8572 labeled images representing drowning, treading water, and swimming\n",
    "3. 7000 randomly chosen for training set, 1572 allocated to validation set\n",
    "4. Every image under images folder has a corresponding text file under labels folder with the same name\n",
    "5. Each text file shows YOLO object detection format: class, x_center, y_center, width, height\n",
    "6. However, the paper does not explicitly define what each class ID (e.g., 0, 1, 2) represents in the drowning detection task.\n",
    "7. Based on a labeled image of someone drowning in the paper and observations in the dataset, we can infer that class 2 represents drowning (vertical position with both hands up).\n",
    "8. Further inference from Figure 5 in the paper suggests that class 0 is swimming and class 1 is tread water\n",
    "\n",
    "# 1.0 Train YOLO Model\n",
    "Train yolo model with the dataset"
   ],
   "id": "a2b374ac7ca1568f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:59:33.533812Z",
     "start_time": "2025-03-07T12:59:27.606830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import yaml\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ],
   "id": "ff07dcb776aae9c0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T12:59:37.328459Z",
     "start_time": "2025-03-07T12:59:37.323824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU detected: {gpu_name}\")"
   ],
   "id": "ab5ee5dab3550ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create dataset.yaml file\n",
    "yaml_content = {\n",
    "    'path': '../',  # Relative path to the root directory\n",
    "    'train': 'data/images/train',  # Training images\n",
    "    'val': 'data/images/val',      # Validation images\n",
    "    'names': {\n",
    "        0: 'person'               # class 0 is 'person'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the YAML file\n",
    "with open('dataset.yaml', 'w') as f:\n",
    "    yaml.dump(yaml_content, f)\n",
    "    print(\"Created dataset.yaml file\")"
   ],
   "id": "9224639aaae33c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the pre-trained model\n",
    "model = YOLO(\"yolo11s.pt\")\n",
    "print(\"Loaded base YOLO model\")"
   ],
   "id": "4fe843fa8f3d93fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "results = model.train(\n",
    "    data='dataset.yaml',\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='human_detection_model',\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
    "    patience=15,  # early stopping patience\n",
    "    save=True,    # save best model\n",
    "    verbose=True\n",
    ")\n",
    "print(\"Training completed\")"
   ],
   "id": "8850965f7ff61a2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model on validation data\n",
    "print(\"Evaluating model...\")\n",
    "metrics = model.val()\n",
    "print(f\"mAP50: {metrics.box.map50}\")\n",
    "print(f\"mAP50-95: {metrics.box.map}\")"
   ],
   "id": "85ddefe1f96b89ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test on a few validation images\n",
    "test_dir = \"../data/images/val\"\n",
    "if os.path.exists(test_dir):\n",
    "    test_images = os.listdir(test_dir)[:5]  # take first 5 images for testing\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, img_name in enumerate(test_images):\n",
    "        img_path = os.path.join(test_dir, img_name)\n",
    "\n",
    "        # Perform prediction\n",
    "        results = model.predict(img_path)\n",
    "\n",
    "        # Plot the image with detected bounding boxes\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Extract and draw bounding boxes\n",
    "        for r in results:\n",
    "            boxes = r.boxes.xyxy.cpu().numpy()\n",
    "            confs = r.boxes.conf.cpu().numpy()\n",
    "\n",
    "            for box, conf in zip(boxes, confs):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(img, f\"Person: {conf:.2f}\", (x1, y1-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{img_name} - Detection\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../models/detection_examples.png')\n",
    "    plt.show()\n",
    "    print(\"Generated detection examples\")"
   ],
   "id": "d357df5b95b0b41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the model\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "save_path = \"../models/human_detection_yolo.pt\"\n",
    "model.export(format=\"pt\", save_dir=\"../models\")\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "id": "27ab9bac90b57d81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nCode block for loading and using the model:\")\n",
    "print(\"\"\"\n",
    "# Load the saved model for inference\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"../models/human_detection_yolo.pt\")\n",
    "\n",
    "# To run on an image\n",
    "results = model.predict(\"path_to_image.jpg\", conf=0.5)\n",
    "\n",
    "# To run on a video\n",
    "results = model.predict(\"path_to_video.mp4\", conf=0.5)\n",
    "\n",
    "# To run on a webcam\n",
    "results = model.predict(0, conf=0.5)  # 0 is the default webcam ID\n",
    "\"\"\")"
   ],
   "id": "92a6d7e230974c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. The code first checks for GPU availability\n",
    "2. It creates a YAML configuration file that defines your dataset structure and class names\n",
    "3. It loads the pre-trained YOLO11s model as a starting point (transfer learning)\n",
    "4. The actual training happens with model.train()\n",
    "5. During training, the model learns to identify people in the images using the labeled data\n",
    "6. Early stopping is implemented to prevent overfitting (stops if no improvement for 15 epochs)\n",
    "7. After training, the model is evaluated on validation data to measure accuracy (mAP metrics)\n",
    "8. The trained model is saved to the models folder for future use in drowning detection\n",
    "9. The code includes visualization to show detection results on a few test images"
   ],
   "id": "fc106d561e84d6bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.0 Drowning Prediction Model\n",
    "1. Pose (Posture) detection : Use MediaPipe\n",
    "2. Drowning Prediction : -\n"
   ],
   "id": "7644a17aeffff9d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ccbdc1dac227c2a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
