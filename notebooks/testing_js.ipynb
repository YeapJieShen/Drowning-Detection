{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JieShen Testing\n",
    "Original -> ROIs -> Training\n",
    "\n",
    "We need these steps:\n",
    "1. Human Detection/Segmentation, seperation of ROIs from frames (With YOLO/YOLO-obb)\n",
    "2. Human Tracking (With SORT)\n",
    "3. Drowning Detection (With our own model e.g. ViT, CKAN, ResNet..... all CNN-based)\n",
    "4. Compute Running Threshold (y_t = \\alpha y_t + (1 - \\alpha) y_(t-1))\n",
    "5. Draw results\n",
    "\n",
    "But most of these steps does not involve any classical image processing steps, so maybe we need to include something like\n",
    "1. Histrogram Equalisation (To remove the blue background, but there are also pretrains can do this)\n",
    "2. Posture Estimation (But not sure how it helps)\n",
    "\n",
    "Do notice that the ground truth given is in [Prediction_Outcome, Bounding Box Coordinates]\n",
    "\n",
    "YeeJing\n",
    "- Say YOLO not good, using nano, suggesting to train from scratch [Suggest to fine-tune by loading the pretrained YOLOs]\n",
    "- In terms of drowning classifier, suggesting to train YOLO-like from scratch [ViT, CKAN, ResNet, YOLO-like...]\n",
    "- How to classify in actual stream deployment\n",
    "- Say not much processing can be done on images, examples like BGR2HSV, resizing, histogram equilisation (focusing on removing/weakening blue channel) [Image Jittering, Rotation]\n",
    "\n",
    "# TODO:\n",
    "1. fine-tune YOLO for human detection\n",
    "2. Try out different preprocessing techniques\n",
    "3. Train different models\n",
    "4. Evaluation of models\n",
    "5. DEPLOYMENT\n",
    "\n",
    "Predicted\n",
    "t_0 = 0\n",
    "t_1 = 0.5\n",
    "t_2 = 0.9\n",
    "t_3 = 1\n",
    "t_4 = 0.3\n",
    "\n",
    "Display (With alpha = 0.5, alpha is the sensitivity in layman terms)\n",
    "The equation: y_t = \\alpha y_t + (1 - \\alpha) y_(t-1)\n",
    "\n",
    "t_0 = 0\n",
    "t_1 = (1 - 0.5) * 0.5 + 0.5 * 0 = 0.25\n",
    "t_2 = (1 - 0.5) * 0.9 + 0.5 * 0.25 = 0.6\n",
    "t_3 = (1 - 0.5) * 0.6 + 0.5 * 1 = 0.8\n",
    "\n",
    "t value is used also to determine the color of the bounding box. [0 - 1]->[Green - Red]\n",
    "Example: If t > 0.7, then emit sound\n",
    "\n",
    "\n",
    "CNN-Based Model:\n",
    "Conv2d\n",
    "Conv2d\n",
    "Pooling\n",
    "Flatten()\n",
    "Softmax/Sigmoid\n",
    "\n",
    "-> Probability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sample by Copilot down here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are instructions along with code cells you might add to your notebook. Create new code cells in your current ipynb and paste each code block:\n",
    "\n",
    "Cell 1 – Import libraries:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's a proposed directory structure for a Python Deep Learning project:\n",
    "\n",
    "\n",
    "\n",
    "Create Workspace..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Cell 2 – Define and compile your CNN:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        # Binary classification: drowning (1) vs normal (0)\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize your model\n",
    "input_shape = (64, 64, 1)  # For grayscale images of size 64x64\n",
    "model = build_cnn_model(input_shape)\n",
    "print(\"CNN Model built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Cell 3 – Process video and run detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video():\n",
    "    # Create a background subtractor\n",
    "    backSub = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=25, detectShadows=True)\n",
    "    cap = cv2.VideoCapture(0)  # Use a webcam. For a video file, provide the file path.\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Frame capture failed, exiting...\")\n",
    "            break\n",
    "\n",
    "        # Apply background subtraction to extract foreground\n",
    "        fgMask = backSub.apply(frame)\n",
    "        \n",
    "        # Resize foreground mask to the required input size for the CNN\n",
    "        resized = cv2.resize(fgMask, (64, 64))\n",
    "        \n",
    "        # Preprocess: normalize and reshape for the CNN\n",
    "        img_input = resized.astype('float32') / 255.0\n",
    "        img_input = np.expand_dims(img_input, axis=-1)  # add channel axis\n",
    "        img_input = np.expand_dims(img_input, axis=0)     # add batch dimension\n",
    "\n",
    "        # Use the CNN for prediction (requires proper training for accuracy)\n",
    "        prediction = model.predict(img_input)\n",
    "        label = \"Drowning\" if prediction[0][0] > 0.5 else \"Normal\"\n",
    "        \n",
    "        # Display label on the original frame\n",
    "        cv2.putText(frame, label, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        cv2.imshow('Live Video', frame)\n",
    "        cv2.imshow('Foreground Mask', fgMask)\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the detection process\n",
    "process_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Simply add these cells in the order shown, and run them in your notebook (ipynb). Adjust parameters and paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO for Human Detection\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Paths to your YOLO configuration, weights, and COCO class names\n",
    "config_path = 'yolov3.cfg'\n",
    "weights_path = 'yolov3.weights'\n",
    "names_path = 'coco.names'\n",
    "\n",
    "# Load COCO class names\n",
    "with open(names_path, 'r') as f:\n",
    "    classes = f.read().strip().split('\\n')\n",
    "\n",
    "# Load YOLO using OpenCV's DNN module\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "def get_yolo_detections(frame):\n",
    "    # Create a blob from the input frame\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Get output layer names\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    layer_outputs = net.forward(ln)\n",
    "    boxes, confidences = [], []\n",
    "    \n",
    "    H, W = frame.shape[:2]\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "            if classes[classID] == \"person\" and confidence > 0.5:\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                centerX, centerY, width, height = box.astype(\"int\")\n",
    "                x = int(centerX - width / 2)\n",
    "                y = int(centerY - height / 2)\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                \n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.5, nms_threshold=0.3)\n",
    "    detections = []\n",
    "    if len(idxs) > 0:\n",
    "        for i in idxs.flatten():\n",
    "            x, y, w_box, h_box = boxes[i]\n",
    "            detections.append((x, y, w_box, h_box))\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SORT for Tracking\n",
    "#  Initialize a CSRT tracker for single object tracking\n",
    "def initialize_tracker(frame, bbox):\n",
    "    tracker = cv2.TrackerCSRT_create()  # CSRT is effective for SOT tasks\n",
    "    tracker.init(frame, bbox)\n",
    "    return tracker\n",
    "\n",
    "# Open a video capture (webcam or file)\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to acquire a frame.\")\n",
    "else:\n",
    "    detections = get_yolo_detections(frame)\n",
    "    if detections:\n",
    "        # Select the first detected person\n",
    "        bbox = tuple(detections[0])\n",
    "        tracker = initialize_tracker(frame, bbox)\n",
    "        print(\"Tracker initialized with bounding box:\", bbox)\n",
    "    else:\n",
    "        print(\"No person detected to initialize tracker.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Build a simple CNN model for drowning classification.\n",
    "def build_drowning_classifier(input_shape=(64, 64, 1)):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "drowning_model = build_drowning_classifier()\n",
    "print(\"Drowning classifier model built successfully.\")\n",
    "\n",
    "# Tracking loop: update tracker, predict drowning status, and display results\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Update the tracker\n",
    "    ok, bbox = tracker.update(frame)\n",
    "    if ok:\n",
    "        (x, y, w_box, h_box) = tuple(map(int, bbox))\n",
    "        cv2.rectangle(frame, (x, y), (x+w_box, y+h_box), (255, 0, 0), 2)\n",
    "        \n",
    "        # Extract region of interest (ROI) for classification\n",
    "        roi = frame[y:y+h_box, x:x+w_box]\n",
    "        if roi.size != 0:\n",
    "            roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            roi_resized = cv2.resize(roi_gray, (64, 64))\n",
    "            roi_norm = roi_resized.astype('float32') / 255.0\n",
    "            roi_input = np.expand_dims(roi_norm, axis=-1)\n",
    "            roi_input = np.expand_dims(roi_input, axis=0)\n",
    "            \n",
    "            # Predict drowning status (this is untrained; train with your data)\n",
    "            prediction = drowning_model.predict(roi_input)\n",
    "            label = \"Drowning\" if prediction[0][0] > 0.5 else \"Normal\"\n",
    "            cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, \"Tracking failure\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        \n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
